{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = pd.read_csv(\"resources/train.csv\",encoding='ISO-8859-1')\n",
    "test_dataset = pd.read_csv(\"resources/test.csv\", encoding='ISO-8859-1')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = train_dataset.drop(['0','1467810369','Mon Apr 06 22:19:45 PDT 2009','NO_QUERY','_TheSpecialOne_'], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = test_dataset.drop(['4','3','Mon May 11 03:17:40 UTC 2009','kindle2','tpryan'], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>@stellargirl I loooooooovvvvvveee my Kindle2. Not that the DX is cool, but the 2 is fantastic in its own right.</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Reading my kindle2...  Love it... Lee childs i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Ok, first assesment of the #kindle2 ...it fuck...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>@kenburbary You'll love your Kindle2. I've had...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>@mikefish  Fair enough. But i have the Kindle2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>@richardebaker no. it is too big. I'm quite ha...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  @stellargirl I loooooooovvvvvveee my Kindle2. Not that the DX is cool, but the 2 is fantastic in its own right.\n",
       "0  Reading my kindle2...  Love it... Lee childs i...                                                             \n",
       "1  Ok, first assesment of the #kindle2 ...it fuck...                                                             \n",
       "2  @kenburbary You'll love your Kindle2. I've had...                                                             \n",
       "3  @mikefish  Fair enough. But i have the Kindle2...                                                             \n",
       "4  @richardebaker no. it is too big. I'm quite ha...                                                             "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    # Remove special characters, numbers, and extra spaces\n",
    "    text = re.sub(r'[^A-Za-z\\s]', '', text)  # Remove special chars and numbers\n",
    "    text = re.sub(r'\\s+', ' ', text)  # Replace multiple spaces with a single space\n",
    "    text = text.strip()  # Remove leading/trailing whitespaces\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_clean_texts = [clean_text(text) for text in train_dataset.iloc[:, 0]]\n",
    "test_clean_texts = [clean_text(text) for text in test_dataset.iloc[:, 0]]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenized Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tokenize_word = [word_tokenize(text) for text in train_clean_texts]\n",
    "train_tokenize_sent = [sent_tokenize(text) for text in train_clean_texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_tokenize_word = [word_tokenize(text) for text in test_clean_texts]\n",
    "test_tokenize_sent = [sent_tokenize(text) for text in test_clean_texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([['is',\n",
       "   'upset',\n",
       "   'that',\n",
       "   'he',\n",
       "   'cant',\n",
       "   'update',\n",
       "   'his',\n",
       "   'Facebook',\n",
       "   'by',\n",
       "   'texting',\n",
       "   'it',\n",
       "   'and',\n",
       "   'might',\n",
       "   'cry',\n",
       "   'as',\n",
       "   'a',\n",
       "   'result',\n",
       "   'School',\n",
       "   'today',\n",
       "   'also',\n",
       "   'Blah'],\n",
       "  ['Kenichan',\n",
       "   'I',\n",
       "   'dived',\n",
       "   'many',\n",
       "   'times',\n",
       "   'for',\n",
       "   'the',\n",
       "   'ball',\n",
       "   'Managed',\n",
       "   'to',\n",
       "   'save',\n",
       "   'The',\n",
       "   'rest',\n",
       "   'go',\n",
       "   'out',\n",
       "   'of',\n",
       "   'bounds'],\n",
       "  ['my',\n",
       "   'whole',\n",
       "   'body',\n",
       "   'feels',\n",
       "   'itchy',\n",
       "   'and',\n",
       "   'like',\n",
       "   'its',\n",
       "   'on',\n",
       "   'fire'],\n",
       "  ['nationwideclass',\n",
       "   'no',\n",
       "   'its',\n",
       "   'not',\n",
       "   'behaving',\n",
       "   'at',\n",
       "   'all',\n",
       "   'im',\n",
       "   'mad',\n",
       "   'why',\n",
       "   'am',\n",
       "   'i',\n",
       "   'here',\n",
       "   'because',\n",
       "   'I',\n",
       "   'cant',\n",
       "   'see',\n",
       "   'you',\n",
       "   'all',\n",
       "   'over',\n",
       "   'there'],\n",
       "  ['Kwesidei', 'not', 'the', 'whole', 'crew']],\n",
       " [['is upset that he cant update his Facebook by texting it and might cry as a result School today also Blah'],\n",
       "  ['Kenichan I dived many times for the ball Managed to save The rest go out of bounds'],\n",
       "  ['my whole body feels itchy and like its on fire'],\n",
       "  ['nationwideclass no its not behaving at all im mad why am i here because I cant see you all over there'],\n",
       "  ['Kwesidei not the whole crew']])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_tokenize_word[:5],train_tokenize_sent[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([['Reading',\n",
       "   'my',\n",
       "   'kindle',\n",
       "   'Love',\n",
       "   'it',\n",
       "   'Lee',\n",
       "   'childs',\n",
       "   'is',\n",
       "   'good',\n",
       "   'read'],\n",
       "  ['Ok',\n",
       "   'first',\n",
       "   'assesment',\n",
       "   'of',\n",
       "   'the',\n",
       "   'kindle',\n",
       "   'it',\n",
       "   'fucking',\n",
       "   'rocks'],\n",
       "  ['kenburbary',\n",
       "   'Youll',\n",
       "   'love',\n",
       "   'your',\n",
       "   'Kindle',\n",
       "   'Ive',\n",
       "   'had',\n",
       "   'mine',\n",
       "   'for',\n",
       "   'a',\n",
       "   'few',\n",
       "   'months',\n",
       "   'and',\n",
       "   'never',\n",
       "   'looked',\n",
       "   'back',\n",
       "   'The',\n",
       "   'new',\n",
       "   'big',\n",
       "   'one',\n",
       "   'is',\n",
       "   'huge',\n",
       "   'No',\n",
       "   'need',\n",
       "   'for',\n",
       "   'remorse'],\n",
       "  ['mikefish',\n",
       "   'Fair',\n",
       "   'enough',\n",
       "   'But',\n",
       "   'i',\n",
       "   'have',\n",
       "   'the',\n",
       "   'Kindle',\n",
       "   'and',\n",
       "   'I',\n",
       "   'think',\n",
       "   'its',\n",
       "   'perfect'],\n",
       "  ['richardebaker',\n",
       "   'no',\n",
       "   'it',\n",
       "   'is',\n",
       "   'too',\n",
       "   'big',\n",
       "   'Im',\n",
       "   'quite',\n",
       "   'happy',\n",
       "   'with',\n",
       "   'the',\n",
       "   'Kindle']],\n",
       " [['Reading my kindle Love it Lee childs is good read'],\n",
       "  ['Ok first assesment of the kindle it fucking rocks'],\n",
       "  ['kenburbary Youll love your Kindle Ive had mine for a few months and never looked back The new big one is huge No need for remorse'],\n",
       "  ['mikefish Fair enough But i have the Kindle and I think its perfect'],\n",
       "  ['richardebaker no it is too big Im quite happy with the Kindle']])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_tokenize_word[:5],test_tokenize_sent[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "train_tokenize_word_cleaned = [\n",
    "    [word.lower() for word in text if word.lower() not in stop_words]\n",
    "    for text in train_tokenize_word\n",
    "]\n",
    "train_tokenize_sent_cleaned = [\n",
    "    [word.lower() for word in sent if word.lower() not in stop_words]\n",
    "    for sent in train_tokenize_sent\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_tokenize_word_cleaned = [\n",
    "    [word.lower() for word in text if word.lower() not in stop_words]\n",
    "    for text in test_tokenize_word\n",
    "]\n",
    "test_tokenize_sent_cleaned = [\n",
    "    [word.lower() for word in sent if word.lower() not in stop_words]\n",
    "    for sent in test_tokenize_sent\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = PorterStemmer()\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "tokenizer = RegexpTokenizer(r'\\w+|[^\\w\\s]+')\n",
    "\n",
    "train_tokenize_word_stemmed = [\n",
    "    [stemmer.stem(word) for word in text] for text in train_tokenize_word_cleaned\n",
    "]\n",
    "train_tokenize_sent_stemmed = [\n",
    "    [stemmer.stem(word) for word in text] for text in train_tokenize_sent_cleaned\n",
    "]\n",
    "\n",
    "train_tokenize_word_lemmatized = [\n",
    "    [lemmatizer.lemmatize(word) for word in text] for text in train_tokenize_word_cleaned\n",
    "]\n",
    "train_tokenize_sent_lemmatized = [\n",
    "    [lemmatizer.lemmatize(word) for word in text] for text in train_tokenize_sent_cleaned\n",
    "]\n",
    "\n",
    "\n",
    "test_tokenize_word_stemmed = [\n",
    "    [stemmer.stem(word) for word in text] for text in test_tokenize_word_cleaned\n",
    "]\n",
    "test_tokenize_sent_stemmed = [\n",
    "    [stemmer.stem(word) for word in text] for text in test_tokenize_sent_cleaned\n",
    "]\n",
    "\n",
    "test_tokenize_word_lemmatized = [\n",
    "    [lemmatizer.lemmatize(word) for word in text] for text in test_tokenize_word_cleaned\n",
    "]\n",
    "test_tokenize_sent_lemmatized = [\n",
    "    [lemmatizer.lemmatize(word) for word in text] for text in test_tokenize_sent_cleaned\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking for emojis in train and test data:\n",
      "No emojis found in train_tokenize_word_stemmed.\n",
      "No emojis found in train_tokenize_sent_stemmed.\n",
      "No emojis found in train_tokenize_word_lemmatized.\n",
      "No emojis found in train_tokenize_sent_lemmatized.\n",
      "No emojis found in test_tokenize_word_stemmed.\n",
      "No emojis found in test_tokenize_sent_stemmed.\n",
      "No emojis found in test_tokenize_word_lemmatized.\n",
      "No emojis found in test_tokenize_sent_lemmatized.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "# Emoji detection function\n",
    "def contains_emoji(s):\n",
    "    emoji_pattern = re.compile(\n",
    "        \"[\\U0001F600-\\U0001F64F\"  # Emoticons\n",
    "        \"\\U0001F300-\\U0001F5FF\"  # Symbols & Pictographs\n",
    "        \"\\U0001F680-\\U0001F6FF\"  # Transport & Map Symbols\n",
    "        \"\\U0001F1E0-\\U0001F1FF\"  # Flags (iOS)\n",
    "        \"\\U00002700-\\U000027BF\"  # Dingbats\n",
    "        \"\\U00002600-\\U000026FF\"  # Misc symbols\n",
    "        \"\\U0001F900-\\U0001F9FF\"  # Supplemental Symbols\n",
    "        \"\\U0001FA70-\\U0001FAFF\"  # Extended Symbols\n",
    "        \"\\U000025A0-\\U000025FF\"  # Geometric Shapes\n",
    "        \"]\",\n",
    "        flags=re.UNICODE\n",
    "    )\n",
    "    return bool(emoji_pattern.search(s))\n",
    "\n",
    "# Function to scan a 2D list for emojis\n",
    "def check_for_emojis(tokenized_data, label):\n",
    "    for i, text in enumerate(tokenized_data):\n",
    "        for word in text:\n",
    "            if contains_emoji(word):\n",
    "                print(f\"Emoji found in {label} at sentence {i}: '{word}'\")\n",
    "                return True\n",
    "    print(f\"No emojis found in {label}.\")\n",
    "    return False\n",
    "\n",
    "# Check train and test tokenized data\n",
    "print(\"Checking for emojis in train and test data:\")\n",
    "\n",
    "# Stemming and Lemmatization\n",
    "check_for_emojis(train_tokenize_word_stemmed, \"train_tokenize_word_stemmed\")\n",
    "check_for_emojis(train_tokenize_sent_stemmed, \"train_tokenize_sent_stemmed\")\n",
    "check_for_emojis(train_tokenize_word_lemmatized, \"train_tokenize_word_lemmatized\")\n",
    "check_for_emojis(train_tokenize_sent_lemmatized, \"train_tokenize_sent_lemmatized\")\n",
    "\n",
    "check_for_emojis(test_tokenize_word_stemmed, \"test_tokenize_word_stemmed\")\n",
    "check_for_emojis(test_tokenize_sent_stemmed, \"test_tokenize_sent_stemmed\")\n",
    "check_for_emojis(test_tokenize_word_lemmatized, \"test_tokenize_word_lemmatized\")\n",
    "check_for_emojis(test_tokenize_sent_lemmatized, \"test_tokenize_sent_lemmatized\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import chain\n",
    "\n",
    "# Flatten each list of sentences into a single list of words\n",
    "train_tokenize_sent_stemmed_flattened = list(chain.from_iterable(train_tokenize_sent_stemmed))\n",
    "train_tokenize_sent_lemmatized_flattened = list(chain.from_iterable(train_tokenize_sent_lemmatized))\n",
    "train_tokenize_word_clean_lemmatized_flattend = list(chain.from_iterable(train_tokenize_word_lemmatized))\n",
    "train_tokenize_word_clean_stemmed_flattend = list(chain.from_iterable(train_tokenize_word_stemmed))\n",
    "\n",
    "\n",
    "test_tokenize_sent_stemmed_flattened = list(chain.from_iterable(test_tokenize_sent_stemmed))\n",
    "test_tokenize_sent_lemmatized_flattened = list(chain.from_iterable(test_tokenize_sent_lemmatized))\n",
    "test_tokenize_word_clean_lemmatized_flattend = list(chain.from_iterable(test_tokenize_word_lemmatized))\n",
    "test_tokenize_word_clean_stemmed_flattend = list(chain.from_iterable(test_tokenize_word_stemmed))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.1 Average Sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average sentence length in Train Sentences Stemmed: 7.72\n",
      "  Runtime: 0.000000 seconds\n",
      "\n",
      "Average sentence length in Train Sentences Lemmatized: 7.72\n",
      "  Runtime: 0.000000 seconds\n",
      "\n",
      "Average sentence length in Test Sentences Stemmed: 8.49\n",
      "  Runtime: 0.000000 seconds\n",
      "\n",
      "Average sentence length in Test Sentences Lemmatized: 8.49\n",
      "  Runtime: 0.000000 seconds\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def average_sentence_length(flattened_words, flattened_sentences):\n",
    "    return len(flattened_words) / len(flattened_sentences) if len(flattened_sentences) > 0 else 0\n",
    "datasets = {\n",
    "    \"Train Sentences Stemmed\": (train_tokenize_word_clean_stemmed_flattend, train_tokenize_sent_stemmed_flattened),\n",
    "    \"Train Sentences Lemmatized\": (train_tokenize_word_clean_lemmatized_flattend, train_tokenize_sent_lemmatized_flattened),\n",
    "    \"Test Sentences Stemmed\": (test_tokenize_word_clean_stemmed_flattend, test_tokenize_sent_stemmed_flattened),\n",
    "    \"Test Sentences Lemmatized\": (test_tokenize_word_clean_lemmatized_flattend, test_tokenize_sent_lemmatized_flattened),\n",
    "}\n",
    "for name, (flattened_words, flattened_sentences) in datasets.items():\n",
    "    start_time = time.time()\n",
    "    avg_length = average_sentence_length(flattened_words, flattened_sentences)\n",
    "    end_time = time.time()  # End timing\n",
    "    runtime = end_time - start_time  # Calculate runtime\n",
    "    print(f\"Average sentence length in {name}: {avg_length:.2f}\")\n",
    "    print(f\"  Runtime: {runtime:.6f} seconds\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.2 Word Sentence Count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Stemming Train set) word count: 12344117 , Sentences Count: 1599962\n",
      "(Lemmatizing Train set) word count: 12344117 , Sentences Count: 1599962\n",
      "(Stemming Test set) word count: 4219 , Sentences Count: 497\n",
      "(Lemmatizing Test set) word count: 4219 , Sentences Count: 497\n",
      "  Runtime: 0.000000 seconds\n",
      "\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "print(f\"(Stemming Train set) word count: {len(train_tokenize_word_clean_stemmed_flattend)} , Sentences Count: {len(train_tokenize_sent_stemmed_flattened)}\")\n",
    "print(f\"(Lemmatizing Train set) word count: {len(train_tokenize_word_clean_lemmatized_flattend)} , Sentences Count: {len(train_tokenize_sent_lemmatized_flattened)}\")\n",
    "print(f\"(Stemming Test set) word count: {len(test_tokenize_word_clean_stemmed_flattend)} , Sentences Count: {len(test_tokenize_sent_stemmed_flattened)}\")\n",
    "print(f\"(Lemmatizing Test set) word count: {len(test_tokenize_word_clean_lemmatized_flattend)} , Sentences Count: {len(test_tokenize_sent_lemmatized_flattened)}\")\n",
    "end_time = time.time()  # End timing\n",
    "runtime = end_time - start_time  # Calculate runtime\n",
    "print(f\"  Runtime: {runtime:.6f} seconds\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.3 Vocabulary size (number of unique words)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unique_word_count(tokens):\n",
    "    return len(set(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Runtime: 1.710448 seconds\n",
      "\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "train_unique_counts = {\n",
    "    \"train_tokenize_sent_stemmed_flattened\": unique_word_count(train_tokenize_sent_stemmed_flattened),\n",
    "    \"train_tokenize_sent_lemmatized_flattened\": unique_word_count(train_tokenize_sent_lemmatized_flattened),\n",
    "    \"train_tokenize_word_clean_lemmatized_flattend\": unique_word_count(train_tokenize_word_clean_lemmatized_flattend),\n",
    "    \"train_tokenize_word_clean_stemmed_flattend\": unique_word_count(train_tokenize_word_clean_stemmed_flattend)\n",
    "}\n",
    "test_unique_counts = {\n",
    "    \"test_tokenize_sent_stemmed_flattened\": unique_word_count(test_tokenize_sent_stemmed_flattened),\n",
    "    \"test_tokenize_sent_lemmatized_flattened\": unique_word_count(test_tokenize_sent_lemmatized_flattened),\n",
    "    \"test_tokenize_word_clean_lemmatized_flattend\": unique_word_count(test_tokenize_word_clean_lemmatized_flattend),\n",
    "    \"test_tokenize_word_clean_stemmed_flattend\": unique_word_count(test_tokenize_word_clean_stemmed_flattend)\n",
    "}\n",
    "end_time = time.time()  # End timing\n",
    "runtime = end_time - start_time  # Calculate runtime\n",
    "print(f\"  Runtime: {runtime:.6f} seconds\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Unique Word Counts:\n",
      "train_tokenize_sent_stemmed_flattened: Unique Word Count = 1566474\n",
      "train_tokenize_sent_lemmatized_flattened: Unique Word Count = 1566849\n",
      "train_tokenize_word_clean_lemmatized_flattend: Unique Word Count = 778982\n",
      "train_tokenize_word_clean_stemmed_flattend: Unique Word Count = 727141\n",
      "\n",
      "Test Unique Word Counts:\n",
      "test_tokenize_sent_stemmed_flattened: Unique Word Count = 497\n",
      "test_tokenize_sent_lemmatized_flattened: Unique Word Count = 497\n",
      "test_tokenize_word_clean_lemmatized_flattend: Unique Word Count = 2035\n",
      "test_tokenize_word_clean_stemmed_flattend: Unique Word Count = 1902\n"
     ]
    }
   ],
   "source": [
    "print(\"Train Unique Word Counts:\")\n",
    "for name, count in train_unique_counts.items():\n",
    "    print(f\"{name}: Unique Word Count = {count}\")\n",
    "\n",
    "print(\"\\nTest Unique Word Counts:\")\n",
    "for name, count in test_unique_counts.items():\n",
    "    print(f\"{name}: Unique Word Count = {count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.4 Max word length, avg. min/max sentence length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def max_word_length(tokens):\n",
    "    return max(len(word) for word in tokens) if tokens else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Runtime: 8.794518 seconds\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Train maximum word length\n",
    "train_max_lengths = {\n",
    "\n",
    "    \"train_tokenize_sent_stemmed_flattened\": max_word_length(train_tokenize_sent_stemmed_flattened),\n",
    "    \"train_tokenize_sent_lemmatized_flattened\": max_word_length(train_tokenize_sent_lemmatized_flattened),\n",
    "    \"train_tokenize_word_clean_lemmatized_flattend\": max_word_length(train_tokenize_word_clean_lemmatized_flattend),\n",
    "    \"train_tokenize_word_clean_stemmed_flattend\": max_word_length(train_tokenize_word_clean_stemmed_flattend)\n",
    "}\n",
    "\n",
    "# Test maximum word length\n",
    "test_max_lengths = {\n",
    "    \"test_tokenize_sent_stemmed_flattened\": max_word_length(test_tokenize_sent_stemmed_flattened),\n",
    "    \"test_tokenize_sent_lemmatized_flattened\": max_word_length(test_tokenize_sent_lemmatized_flattened),\n",
    "    \"test_tokenize_word_clean_lemmatized_flattend\": max_word_length(test_tokenize_word_clean_lemmatized_flattend),\n",
    "    \"test_tokenize_word_clean_stemmed_flattend\": max_word_length(test_tokenize_word_clean_stemmed_flattend)\n",
    "}\n",
    "end_time = time.time()  # End timing\n",
    "runtime = end_time - start_time  # Calculate runtime\n",
    "print(f\"  Runtime: {runtime:.6f} seconds\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Max Word Lengths:\n",
      "train_tokenize_sent_stemmed_flattened: Max Word Length = 176\n",
      "train_tokenize_sent_lemmatized_flattened: Max Word Length = 177\n",
      "train_tokenize_word_clean_lemmatized_flattend: Max Word Length = 125\n",
      "train_tokenize_word_clean_stemmed_flattend: Max Word Length = 123\n",
      "\n",
      "Test Max Word Lengths:\n",
      "test_tokenize_sent_stemmed_flattened: Max Word Length = 138\n",
      "test_tokenize_sent_lemmatized_flattened: Max Word Length = 139\n",
      "test_tokenize_word_clean_lemmatized_flattend: Max Word Length = 46\n",
      "test_tokenize_word_clean_stemmed_flattend: Max Word Length = 46\n"
     ]
    }
   ],
   "source": [
    "print(\"Train Max Word Lengths:\")\n",
    "for name, length in train_max_lengths.items():\n",
    "    print(f\"{name}: Max Word Length = {length}\")\n",
    "\n",
    "print(\"\\nTest Max Word Lengths:\")\n",
    "for name, length in test_max_lengths.items():\n",
    "    print(f\"{name}: Max Word Length = {length}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def min_max_sentence_length(sentences):\n",
    "    sentence_lengths = [len(sentence) for sentence in sentences]  # List of sentence lengths\n",
    "    if sentence_lengths:\n",
    "        return min(sentence_lengths), max(sentence_lengths)  # Min and max sentence lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Runtime: 0.000000 seconds\n",
      "\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "train_min_max_lengths = {\n",
    "    \"train_tokenize_sent_stemmed_flattened\": min_max_sentence_length(train_tokenize_sent_stemmed_flattened),\n",
    "    \"train_tokenize_sent_lemmatized_flattened\": min_max_sentence_length(train_tokenize_sent_lemmatized_flattened),\n",
    "    \"train_tokenize_word_clean_lemmatized_flattend\": min_max_sentence_length(train_tokenize_word_clean_lemmatized_flattend),\n",
    "    \"train_tokenize_word_clean_stemmed_flattend\": min_max_sentence_length(train_tokenize_word_clean_stemmed_flattend)\n",
    "}\n",
    "\n",
    "test_min_max_lengths = {\n",
    "    \"test_tokenize_sent_stemmed_flattened\": min_max_sentence_length(test_tokenize_sent_stemmed_flattened),\n",
    "    \"test_tokenize_sent_lemmatized_flattened\": min_max_sentence_length(test_tokenize_sent_lemmatized_flattened),\n",
    "    \"test_tokenize_word_clean_lemmatized_flattend\": min_max_sentence_length(test_tokenize_word_clean_lemmatized_flattend),\n",
    "    \"test_tokenize_word_clean_stemmed_flattend\": min_max_sentence_length(test_tokenize_word_clean_stemmed_flattend)\n",
    "}\n",
    "end_time = time.time()  # End timing\n",
    "runtime = end_time - start_time  # Calculate runtime\n",
    "print(f\"  Runtime: {runtime:.6f} seconds\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Min and Max Sentence Lengths:\n",
      "train_tokenize_sent_stemmed_flattened: Min Sentence Length = 2, Max Sentence Length = 176\n",
      "train_tokenize_sent_lemmatized_flattened: Min Sentence Length = 2, Max Sentence Length = 177\n",
      "train_tokenize_word_clean_lemmatized_flattend: Min Sentence Length = 1, Max Sentence Length = 125\n",
      "train_tokenize_word_clean_stemmed_flattend: Min Sentence Length = 1, Max Sentence Length = 123\n",
      "\n",
      "Test Min and Max Sentence Lengths:\n",
      "test_tokenize_sent_stemmed_flattened: Min Sentence Length = 10, Max Sentence Length = 138\n",
      "test_tokenize_sent_lemmatized_flattened: Min Sentence Length = 11, Max Sentence Length = 139\n",
      "test_tokenize_word_clean_lemmatized_flattend: Min Sentence Length = 1, Max Sentence Length = 46\n",
      "test_tokenize_word_clean_stemmed_flattend: Min Sentence Length = 1, Max Sentence Length = 46\n"
     ]
    }
   ],
   "source": [
    "print(\"Train Min and Max Sentence Lengths:\")\n",
    "for name, (min_len, max_len) in train_min_max_lengths.items():\n",
    "    print(f\"{name}: Min Sentence Length = {min_len}, Max Sentence Length = {max_len}\")\n",
    "\n",
    "print(\"\\nTest Min and Max Sentence Lengths:\")\n",
    "for name, (min_len, max_len) in test_min_max_lengths.items():\n",
    "    print(f\"{name}: Min Sentence Length = {min_len}, Max Sentence Length = {max_len}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.5 #emoticon removed, # stop word removed, # token count, # lowercase , # special char removed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text_with_runtime(data):\n",
    "    \"\"\"\n",
    "    Function to preprocess text from the first column of a DataFrame and measure runtime.\n",
    "\n",
    "    Args:\n",
    "        data (DataFrame): Input DataFrame containing text data.\n",
    "\n",
    "    Returns:\n",
    "        tuple: A list of dictionaries with preprocessing metrics and the runtime in seconds.\n",
    "    \"\"\"\n",
    "    start_time = time.time()  # Start timing\n",
    "\n",
    "    results_list = []\n",
    "\n",
    "    for text in data.iloc[:, 0]:  # Access the first column\n",
    "        # Initialize counters\n",
    "        phone_count = 0\n",
    "        account_count = 0\n",
    "        address_count = 0\n",
    "\n",
    "        # 1. Remove emoticons\n",
    "        emoticon_pattern = re.compile(\"[\\U0001F600-\\U0001F64F\\U0001F300-\\U0001F5FF\\U0001F680-\\U0001F6FF\\U0001F1E0-\\U0001F1FF]\")\n",
    "        emoticons = emoticon_pattern.findall(text)\n",
    "        text_no_emoticons = emoticon_pattern.sub(\"\", text)\n",
    "\n",
    "        # 2. Check for phone numbers (basic pattern: xxx-xxx-xxxx or (xxx) xxx-xxxx or xxx.xxx.xxxx)\n",
    "        phone_pattern = re.compile(r\"\\b\\d{3}[-.\\s]?\\d{3}[-.\\s]?\\d{4}\\b\")\n",
    "        phone_count = len(phone_pattern.findall(text_no_emoticons))\n",
    "\n",
    "        # 3. Check for account numbers (pattern: 8-20 digit numbers)\n",
    "        account_pattern = re.compile(r\"\\b\\d{8,20}\\b\")\n",
    "        account_count = len(account_pattern.findall(text_no_emoticons))\n",
    "\n",
    "        # 4. Check for addresses (basic heuristic: numbers followed by words)\n",
    "        address_pattern = re.compile(r\"\\b\\d+\\s+[A-Za-z]+\\s+[A-Za-z]+\")\n",
    "        address_count = len(address_pattern.findall(text_no_emoticons))\n",
    "\n",
    "        # 5. Tokenize text\n",
    "        tokens = word_tokenize(text_no_emoticons)\n",
    "\n",
    "        # 6. Remove stop words\n",
    "        stop_words = set(stopwords.words('english'))\n",
    "        tokens_no_stopwords = [word for word in tokens if word.lower() not in stop_words]\n",
    "        stopwords_removed = len(tokens) - len(tokens_no_stopwords)\n",
    "\n",
    "        # 7. Count tokens\n",
    "        token_count = len(tokens_no_stopwords)\n",
    "\n",
    "        # 8. Convert to lowercase\n",
    "        tokens_lowercase = [word.lower() for word in tokens_no_stopwords]\n",
    "\n",
    "        # 9. Remove special characters\n",
    "        special_chars = re.findall(r\"[^\\w\\s]\", \" \".join(tokens_lowercase))\n",
    "        text_no_special_chars = re.sub(r\"[^\\w\\s]\", \"\", \" \".join(tokens_lowercase))\n",
    "\n",
    "        # Results for this text\n",
    "        results = {\n",
    "            \"Emoticons Removed\": len(emoticons),\n",
    "            \"Stop Words Removed\": stopwords_removed,\n",
    "            \"Token Count\": token_count,\n",
    "            \"Lowercase Applied\": text_no_special_chars,\n",
    "            \"Special Characters Removed\": len(special_chars),\n",
    "            \"Phone Numbers Found\": phone_count,\n",
    "            \"Account Numbers Found\": account_count,\n",
    "            \"Addresses Found\": address_count,\n",
    "        }\n",
    "\n",
    "        results_list.append(results)\n",
    "\n",
    "    end_time = time.time()  # End timing\n",
    "    runtime = end_time - start_time  # Calculate runtime\n",
    "\n",
    "    return results_list, runtime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Results:\n",
      "Row 1: {'Emoticons Removed': 0, 'Stop Words Removed': 9, 'Token Count': 16, 'Lowercase Applied': 'upset ca nt update facebook texting  might cry result school today also  blah ', 'Special Characters Removed': 6, 'Phone Numbers Found': 0, 'Account Numbers Found': 0, 'Addresses Found': 0}\n",
      "Row 2: {'Emoticons Removed': 0, 'Stop Words Removed': 7, 'Token Count': 14, 'Lowercase Applied': ' kenichan dived many times ball  managed save 50  rest go bounds', 'Special Characters Removed': 3, 'Phone Numbers Found': 0, 'Account Numbers Found': 0, 'Addresses Found': 0}\n",
      "Row 3: {'Emoticons Removed': 0, 'Stop Words Removed': 4, 'Token Count': 6, 'Lowercase Applied': 'whole body feels itchy like fire', 'Special Characters Removed': 0, 'Phone Numbers Found': 0, 'Account Numbers Found': 0, 'Addresses Found': 0}\n",
      "Row 4: {'Emoticons Removed': 0, 'Stop Words Removed': 16, 'Token Count': 14, 'Lowercase Applied': ' nationwideclass  s behaving  m mad   ca nt see ', 'Special Characters Removed': 9, 'Phone Numbers Found': 0, 'Account Numbers Found': 0, 'Addresses Found': 0}\n",
      "Row 5: {'Emoticons Removed': 0, 'Stop Words Removed': 2, 'Token Count': 4, 'Lowercase Applied': ' kwesidei whole crew', 'Special Characters Removed': 1, 'Phone Numbers Found': 0, 'Account Numbers Found': 0, 'Addresses Found': 0}\n",
      "Train Processing Runtime: 403.0391 seconds\n",
      "\n",
      "Test Results:\n",
      "Row 1: {'Emoticons Removed': 0, 'Stop Words Removed': 3, 'Token Count': 10, 'Lowercase Applied': 'reading kindle2  love  lee childs good read ', 'Special Characters Removed': 7, 'Phone Numbers Found': 0, 'Account Numbers Found': 0, 'Addresses Found': 0}\n",
      "Row 2: {'Emoticons Removed': 0, 'Stop Words Removed': 3, 'Token Count': 12, 'Lowercase Applied': 'ok  first assesment  kindle2  fucking rocks   ', 'Special Characters Removed': 8, 'Phone Numbers Found': 0, 'Account Numbers Found': 0, 'Addresses Found': 0}\n",
      "Row 3: {'Emoticons Removed': 0, 'Stop Words Removed': 12, 'Token Count': 23, 'Lowercase Applied': ' kenburbary ll love kindle2  ve mine months never looked back  new big one huge  need remorse   ', 'Special Characters Removed': 9, 'Phone Numbers Found': 0, 'Account Numbers Found': 0, 'Addresses Found': 0}\n",
      "Row 4: {'Emoticons Removed': 0, 'Stop Words Removed': 7, 'Token Count': 11, 'Lowercase Applied': ' mikefish fair enough  kindle2 think s perfect  ', 'Special Characters Removed': 5, 'Phone Numbers Found': 0, 'Account Numbers Found': 0, 'Addresses Found': 0}\n",
      "Row 5: {'Emoticons Removed': 0, 'Stop Words Removed': 7, 'Token Count': 10, 'Lowercase Applied': ' richardebaker  big  m quite happy kindle2 ', 'Special Characters Removed': 5, 'Phone Numbers Found': 0, 'Account Numbers Found': 0, 'Addresses Found': 0}\n",
      "Test Processing Runtime: 0.1262 seconds\n"
     ]
    }
   ],
   "source": [
    "train_results, train_runtime = preprocess_text_with_runtime(train_dataset)\n",
    "test_results , test_runtime = preprocess_text_with_runtime(test_dataset)\n",
    "# Print the train results\n",
    "print(\"Train Results:\")\n",
    "for i, result in enumerate(train_results[:5]):  # Print the first 5 rows for brevity\n",
    "    print(f\"Row {i + 1}: {result}\")\n",
    "\n",
    "# Print the train runtime\n",
    "print(f\"Train Processing Runtime: {train_runtime:.4f} seconds\\n\")\n",
    "\n",
    "# Print the test results\n",
    "print(\"Test Results:\")\n",
    "for i, result in enumerate(test_results[:5]):  # Print the first 5 rows for brevity\n",
    "    print(f\"Row {i + 1}: {result}\")\n",
    "\n",
    "# Print the test runtime\n",
    "print(f\"Test Processing Runtime: {test_runtime:.4f} seconds\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Readability Scores (e.g., Flesch-Kincaid): Ensure text remains interpretable\n",
    "# Lexical Diversity: Ratio of unique words to total words. ( read and use)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "75.54\n",
      "96.52\n",
      "112.09\n",
      "75.54\n",
      "117.16\n",
      "119.19\n",
      "  Runtime: 26.633111 seconds\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import textstat\n",
    "train_stemmed_scores = []\n",
    "start_time = time.time()  # Start timing\n",
    "\n",
    "for sentence in train_tokenize_sent_stemmed_flattened:\n",
    "    score = textstat.flesch_reading_ease(sentence)\n",
    "    train_stemmed_scores.append(score)\n",
    "\n",
    "for i in range(6):\n",
    "    print(train_stemmed_scores[i])\n",
    "    \n",
    "end_time = time.time()  # End timing\n",
    "runtime = end_time - start_time  # Calculate runtime\n",
    "print(f\"  Runtime: {runtime:.6f} seconds\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Lexical Diversity: 0.0589\n",
      "Train Runtime: 0.700918 seconds\n",
      "\n",
      "Test Lexical Diversity: 0.4508\n",
      "Test Runtime: 0.001000 seconds\n"
     ]
    }
   ],
   "source": [
    "def lexical_diversity(flattened_words):\n",
    "\n",
    "    total_words = len(flattened_words)\n",
    "    unique_words = len(set(flattened_words))  # Using set to get unique words\n",
    "    return unique_words / total_words if total_words > 0 else 0\n",
    "\n",
    "# Measure time for train lexical diversity calculation\n",
    "start_time_train = time.time()  # Start timing\n",
    "train_lexical_diversity = lexical_diversity(train_tokenize_word_clean_stemmed_flattend)\n",
    "end_time_train = time.time()  # End timing\n",
    "train_runtime = end_time_train - start_time_train  # Calculate runtime\n",
    "\n",
    "# Measure time for test lexical diversity calculation\n",
    "start_time_test = time.time()  # Start timing\n",
    "test_lexical_diversity = lexical_diversity(test_tokenize_word_clean_stemmed_flattend)\n",
    "end_time_test = time.time()  # End timing\n",
    "test_runtime = end_time_test - start_time_test  # Calculate runtime\n",
    "\n",
    "# Print results\n",
    "print(f\"Train Lexical Diversity: {train_lexical_diversity:.4f}\")\n",
    "print(f\"Train Runtime: {train_runtime:.6f} seconds\\n\")\n",
    "\n",
    "print(f\"Test Lexical Diversity: {test_lexical_diversity:.4f}\")\n",
    "print(f\"Test Runtime: {test_runtime:.6f} seconds\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
